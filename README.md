# LSC Chatbot Base

Build a local, private chatbot trained on your own documents using [Ollama](https://ollama.com), [nomic-embed-text](https://ollama.com/library/nomic-embed-text) for embeddings, and local LLMs like [Qwen3](https://huggingface.co/qwen), [Nous Hermes 2 Yi](https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B), or [LLaMA 3](https://ollama.com/library/llama3) for chat responses.

---

## ğŸš€ Features
- âœ… Train on your own `.txt` files
- ğŸ” Search using vector embeddings (`nomic-embed-text`)
- ğŸ’¬ Chat with context using local models like `qwen3`, `qwen3`, `llama3`, `mistral`, `phi`, or `gemma`
- âš¡ Fast local inference via Ollama (for supported models)
- ğŸ” Dynamically switch models in real-time (`/switch mistral`)
- ğŸ–¥ï¸ CLI and FastAPI interface

---

## ğŸ“ Project Structure
```

lsc\_chat\_bot\_base/
â”œâ”€â”€ data\_loader.py       # loads .txt files from /data
â”œâ”€â”€ embeddings.py        # uses nomic-embed-text to embed documents
â”œâ”€â”€ vector\_db.py         # stores/query embeddings with ChromaDB
â”œâ”€â”€ bot.py               # CLI chatbot interface
â”œâ”€â”€ api\_server.py        # REST API using FastAPI
â”œâ”€â”€ environment.yml      # required packages
â”œâ”€â”€ .env                 # environment variables
â”œâ”€â”€ data/                # your local document directory
â””â”€â”€ README.md            # you're here

````

---

## âš™ï¸ Setup Instructions

### 1. Install Ollama
Download from: [https://ollama.com](https://ollama.com)  
Then start it:
```bash
ollama serve
````

### 2. Pull Required Models

Supported by Ollama:

```bash
ollama pull nomic-embed-text
ollama pull llama3         # or mistral, phi, gemma, nous-hermes2-yi
```

> âš ï¸ **Qwen3 is currently NOT available via `ollama pull`.**
> To use Qwen3, run it locally via Hugging Face weights and an inference server like [text-generation-webui](https://github.com/oobabooga/text-generation-webui) or [vLLM](https://github.com/vllm-project/vllm).

---

### 3. Clone the Repo

```bash
git clone https://github.com/lscblack/lsc_chat_bot_base.git
cd lsc_chat_bot_base
```

### 4. Install Python Dependencies

```bash
pip install -r requirements.txt
```

### 5. Add Your Data

Put `.txt` files into the `data/` directory. Example:

```
data/
â”œâ”€â”€ example1.txt
â””â”€â”€ notes.txt
```

### 6. Run the CLI Chatbot

```bash
python bot.py
```

You'll see:

```
[Default model: nous-hermes2-yi]
You: 
```

ğŸ’¡ To switch models in chat:

```
/switch mistral
```

---

## ğŸŒ Run the FastAPI Server

```bash
uvicorn api_server:app --reload
```

Then open [http://localhost:8000/docs](http://localhost:8000/docs) to test the API.

### Sample Request:

```
POST /ask
{
  "question": "What is this bot about?"
}
```

---

## ğŸ”„ Available Commands (CLI)

| Command           | Description               |
| ----------------- | ------------------------- |
| `/switch <model>` | Switch to a different LLM |
| `exit` or `quit`  | End the session           |

---

## ğŸ¤– Models You Can Use

| Model           | Quality         | Speed       | Use For                                |
| --------------- | --------------- | ----------- | -------------------------------------- |
| nous-hermes2-yi | ğŸ§  Multilingual | Medium      | Deep knowledge, many languages         |
| qwen3 (local)   | ğŸ§  Multilingual | Medium-Fast | Strong multilingual, coding, reasoning |
| llama3          | ğŸ§  Best Q\&A    | Medium      | Deep reasoning                         |
| mistral         | âš¡ Fast          | Fast        | Chatbots, productivity, devs           |
| phi             | ğŸª¶ Light        | Very Fast   | Basic replies, small resource use      |
| gemma           | ğŸŒ Balanced     | Fast        | Academic/general answers               |

To see what models you've pulled via Ollama:

```bash
ollama list
```

---

## ğŸ§ª Example Flow

```
You: How do I train this bot?
Bot: Just add your text files into /data and run bot.py. The system embeds them with nomic-embed-text, stores them in a vector DB, and uses Nous Hermes 2 Yi to generate answers based on the most relevant chunks.

You: /switch mistral
Bot: [Switched to model: mistral]
```

---

## ğŸ“¦ Deployment Suggestions

* Deploy the FastAPI app behind Nginx or serve via `gunicorn`
* Use systemd to keep `ollama` and `uvicorn` running
* Optionally expose a web UI (React/Flutter)

---

## ğŸ‘¨â€ğŸ’» Author

**Loue Sauveur Christian (lscblack)**
Software Engineer | AI Explorer | Entrepreneur

GitHub: [@lscblack](https://github.com/lscblack)

---

## ğŸ“œ License

MIT License â€” free to use, modify, share.

